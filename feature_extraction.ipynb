{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5C0d-Epi4qZM"
      },
      "outputs": [],
      "source": [
        "# Input: an array consisting of 100 word text sections\n",
        "# Output: BOW vectorized sections"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "lvhT5cZj419c"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bag_of_words(texts):\n",
        "    vectorizer = CountVectorizer()\n",
        "    X = vectorizer.fit_transform(texts)\n",
        "    return X.toarray()\n",
        "\n"
      ],
      "metadata": {
        "id": "RI1nJ2eRnya7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def n_grams(texts, n=2):\n",
        "    vectorizer = CountVectorizer(ngram_range=(n, n))\n",
        "    X = vectorizer.fit_transform(texts)\n",
        "    return X.toarray()\n",
        "\n"
      ],
      "metadata": {
        "id": "pU78RIxmnyU3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tfidf(texts):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X = vectorizer.fit_transform(texts)\n",
        "    return X.toarray()\n",
        "\n"
      ],
      "metadata": {
        "id": "lzno2FfQnyOg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lda(texts, n_topics=5):\n",
        "    vectorizer = CountVectorizer()\n",
        "    X = vectorizer.fit_transform(texts)\n",
        "    lda_model = LatentDirichletAllocation(n_components=n_topics)\n",
        "    lda_features = lda_model.fit_transform(X)\n",
        "    return lda_features"
      ],
      "metadata": {
        "id": "ziLg2yw1nyAa"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_df_from_json(json_file_path):\n",
        "    try:\n",
        "        df = pd.read_json(json_file_path)\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return []\n",
        "\n",
        "\n",
        "def get_string_list_from_df(df, column_name):\n",
        "    try:\n",
        "        # Convert column to list and ensure all elements are strings\n",
        "        string_list = df[column_name].astype(str).tolist()\n",
        "        return string_list\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return []\n"
      ],
      "metadata": {
        "id": "uWGDaqnFolqc"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tx = [\"ces dae\", \"cis sda cis\", \"cis ar\"]\n",
        "print(bag_of_words(tx))\n",
        "print(n_grams(tx, 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2C_Inx3Do9KR",
        "outputId": "a4c984d2-2aec-4596-e2de-dba304f58317"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 1 0 1 0]\n",
            " [0 0 2 0 1]\n",
            " [1 0 1 0 0]]\n",
            "[[1 0 0 0]\n",
            " [0 0 1 1]\n",
            " [0 1 0 0]]\n"
          ]
        }
      ]
    }
  ]
}