{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21d96bf7-eae3-4d14-81fe-c3a371601a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering complete. All features saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "import joblib\n",
    "\n",
    "# Load the Excel file\n",
    "sampled_labeled_partitions = pd.read_csv('cleaned_labeled_partitions.csv')\n",
    "\n",
    "# === Original teammate functions ===\n",
    "def bag_of_words(texts):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    return X.toarray(), vectorizer\n",
    "\n",
    "def n_grams(texts, n=2):\n",
    "    vectorizer = CountVectorizer(ngram_range=(n, n))\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    return X.toarray(), vectorizer\n",
    "\n",
    "def tfidf(texts):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    return X.toarray(), vectorizer\n",
    "\n",
    "def lda_encoding(texts, n_topics=5):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    lda_model = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "    lda_features = lda_model.fit_transform(X)\n",
    "    return lda_features, lda_model, vectorizer\n",
    "\n",
    "def word2vec_encoding(texts, vector_size=100, window=5, min_count=1):\n",
    "    tokenized_texts = [simple_preprocess(doc) for doc in texts]\n",
    "    model = Word2Vec(sentences=tokenized_texts, vector_size=vector_size, window=window,\n",
    "                     min_count=min_count, workers=4, seed=42)\n",
    "    doc_vectors = []\n",
    "    for tokens in tokenized_texts:\n",
    "        vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "        if vectors:\n",
    "            doc_vectors.append(np.mean(vectors, axis=0))\n",
    "        else:\n",
    "            doc_vectors.append(np.zeros(vector_size))\n",
    "    return np.array(doc_vectors), model\n",
    "\n",
    "# === Load the sampled data ===\n",
    "# sampled_labeled_partitions should already exist in your notebook\n",
    "texts = sampled_labeled_partitions['text'].tolist()\n",
    "labels = sampled_labeled_partitions['label']\n",
    "numeric_labels = pd.factorize(labels)[0]\n",
    "\n",
    "# === Apply features ===\n",
    "# Bag of Words\n",
    "bow_matrix, bow_vectorizer = bag_of_words(texts)\n",
    "df_bow = pd.DataFrame(bow_matrix, columns=[f'bow_{i}' for i in range(bow_matrix.shape[1])])\n",
    "df_bow['label'] = labels\n",
    "df_bow['label_num'] = numeric_labels\n",
    "df_bow.to_pickle('bow.pkl')\n",
    "joblib.dump(bow_vectorizer, 'bow_vectorizer.pkl')\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_matrix, tfidf_vectorizer = tfidf(texts)\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix, columns=[f'tfidf_{i}' for i in range(tfidf_matrix.shape[1])])\n",
    "df_tfidf['label'] = labels\n",
    "df_tfidf['label_num'] = numeric_labels\n",
    "df_tfidf.to_pickle('tfidf.pkl')\n",
    "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')\n",
    "\n",
    "# N-grams (bigrams)\n",
    "ngram_matrix, ngram_vectorizer = n_grams(texts, n=2)\n",
    "df_ngram = pd.DataFrame(ngram_matrix, columns=[f'ngram_{i}' for i in range(ngram_matrix.shape[1])])\n",
    "df_ngram['label'] = labels\n",
    "df_ngram['label_num'] = numeric_labels\n",
    "df_ngram.to_pickle('ngram.pkl')\n",
    "joblib.dump(ngram_vectorizer, 'ngram_vectorizer.pkl')\n",
    "\n",
    "# LDA\n",
    "lda_matrix, lda_model, lda_vectorizer = lda_encoding(texts, n_topics=5)\n",
    "df_lda = pd.DataFrame(lda_matrix, columns=[f'lda_topic_{i}' for i in range(lda_matrix.shape[1])])\n",
    "df_lda['label'] = labels\n",
    "df_lda['label_num'] = numeric_labels\n",
    "df_lda.to_pickle('lda.pkl')\n",
    "joblib.dump(lda_model, 'lda_model.pkl')\n",
    "joblib.dump(lda_vectorizer, 'lda_vectorizer.pkl')\n",
    "\n",
    "# Word2Vec\n",
    "w2v_matrix, w2v_model = word2vec_encoding(texts, vector_size=100, window=5, min_count=1)\n",
    "df_w2v = pd.DataFrame(w2v_matrix, columns=[f'w2v_{i}' for i in range(w2v_matrix.shape[1])])\n",
    "df_w2v['label'] = labels\n",
    "df_w2v['label_num'] = numeric_labels\n",
    "df_w2v.to_pickle('word2vec.pkl')\n",
    "w2v_model.save('word2vec.model')\n",
    "\n",
    "print(\"Feature engineering complete. All features saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febf66e3-b39d-438b-b5ec-e8a5998637b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
